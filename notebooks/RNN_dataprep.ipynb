{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b558342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9af1bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERNAL FUNCS\n",
    "def splitdata(data, label, ntrainbatch=10):\n",
    "    nbatch=ntrainbatch-1\n",
    "    trainframes=[]\n",
    "    testframes =[]\n",
    "    for i in range(45):\n",
    "        if i%15-1<nbatch:\n",
    "            trainframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "        if i%15>nbatch:\n",
    "            testframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "    train = pd.concat(trainframes)\n",
    "    test  = pd.concat(testframes)\n",
    "    return train, test\n",
    "\n",
    "def gatherdata(X, y):\n",
    "    Xyframes=[]\n",
    "    for i in range(45):\n",
    "        Xyframes.append(pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1))\n",
    "    XyDF = pd.concat(Xyframes)\n",
    "    return XyDF\n",
    "\n",
    "def allsets(X,y,slice_size=13, trackdict=False):\n",
    "    slices = []\n",
    "    dicc={}\n",
    "    for i in range(45):\n",
    "        conc = pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1)\n",
    "        length   = len(conc)\n",
    "        sobrantes, setscomp = length%slice_size, length//slice_size\n",
    "        \n",
    "        for e in range(setscomp):\n",
    "            slic = conc.iloc[slice_size*e:min_size*e+slice_size]\n",
    "            slices.append(slic)\n",
    "            dicc[f\"clip {i}\"]=f\"slices:{setscomp}\" #dicc {clip,slice}\n",
    "    df = pd.concat(slices)\n",
    "    if trackdict:\n",
    "        return df, dicc\n",
    "    if not trackdict:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c50e602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL FUNCS\n",
    "\n",
    "#Train-Test from Full DF function \n",
    "def fulldfsplit(nsubjects=16):\n",
    "    '''Files must be labelled as {subject#}_123.npz' and should be inside a Data folder within the Project'''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    Xytrain16_list = []\n",
    "    Xytest16_list  = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "            \n",
    "    for i in range(1,nsubjects+1):\n",
    "        #apply all data to the splitdata func to create lists of DFs \n",
    "        train, test = splitdata(data16[i], label16[i], 10)\n",
    "        Xytrain16_list.append(train)\n",
    "        Xytest16_list.append(test)\n",
    "\n",
    "    #create a unified DF from every list with pd.concat(trainframes)\n",
    "    Xytrain16_DF = pd.concat(Xytrain16_list)\n",
    "    Xytest16_DF  = pd.concat(Xytest16_list)\n",
    "    \n",
    "    return Xytrain16_DF, Xytest16_DF\n",
    "\n",
    "#Full DF no split\n",
    "def fulldf(nsubjects=16):\n",
    "    '''Files must be labelled as {subject#}_123.npz' and should be inside a Data folder within the Project.'''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "    Xy16_list = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        #apply all data to the gather data func to create lists of DFs \n",
    "        Xy = gatherdata(data16[i], label16[i])\n",
    "        Xy16_list.append(Xy)\n",
    "    XyDF = pd.concat(Xy16_list)\n",
    "    XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return XyDF\n",
    "\n",
    "#Full DF, no split, slicing each clip to multiples of 13\n",
    "def fulldfslices(nsubjects=16, slice_size =13, trackdict=False):\n",
    "    '''Files must be labelled as {subject#}_123.npz' and should be inside a Data folder within the Project.\n",
    "    slice_size is the desired row length of each slice\n",
    "    '''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    #trackdic = trackdict\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "    Xy16_list = []\n",
    "    dicc16  = {}\n",
    "    for i in range(1,nsubjects+1): \n",
    "        #apply all data to the gather data func to create lists of DFs \n",
    "        if trackdict:\n",
    "            Xy,dicc = allsets(data16[i], label16[i], slice_size, trackdict=True)\n",
    "            Xy16_list.append(Xy)\n",
    "            dicc16[f\"subject {i}\"]=dicc  #list with dicc {clip,slice}\n",
    "        if not trackdict:\n",
    "            Xy = allsets(data16[i], label16[i], slice_size)\n",
    "            Xy16_list.append(Xy)\n",
    "\n",
    "    XyDF = pd.concat(Xy16_list)\n",
    "    XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return XyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef2e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dayus formula\n",
    "def get_X_y(df, \n",
    "            X_length=13, # \n",
    "            y_length=13, \n",
    "            number_of_sequences=51, \n",
    "            number_of_targets=1, \n",
    "            val=False, \n",
    "            val_cutoff=0.8):\n",
    "\n",
    "\n",
    "#     limit dataframes to length for train/test splits\n",
    "    df_X = df.copy().drop(columns=df.columns[-1], axis=1)\n",
    "    df_y = df.iloc[:, -1].copy()\n",
    "    \n",
    "#     convert and scale X dataframe to PCA to solve dimensionality problem\n",
    "    scaler = MinMaxScaler()\n",
    "    df_X_scaled = pd.DataFrame(scaler.fit_transform(df_X), columns=df_X.columns, index=df_X.index)\n",
    "    \n",
    "\n",
    "#     create unique list to sample random datapoints from\n",
    "    if val:\n",
    "        sample_list = list(range(int(len(df_y)*val_cutoff), int(len(df_y)-y_length))) #y_length pans the end\n",
    "    if not val:\n",
    "        sample_list = list(range(int(X_length), int(len(df_y)-y_length))) #X_length pans start\n",
    "    random.shuffle(sample_list)\n",
    "    \n",
    "#     empty lists to append data to, will create 3D dataframe here\n",
    "    X, y = [], []\n",
    "    \n",
    "    \n",
    "#     define a simple data slicing and selection function. This function will create a slice of data from a specified random starting position. The random position must be generated externally.\n",
    "    \n",
    "    def get_Xi_yi(df_X, \n",
    "              df_y,\n",
    "              random_start, #list of random values\n",
    "              X_length, #X_length pans start\n",
    "              y_length #y_length pans the end\n",
    "                 ): \n",
    "        '''Define a simple data slicing and selection function. \n",
    "        This function will create a slice of data from a specified random starting position. \n",
    "        The random position must be generated externally.'''\n",
    "    \n",
    "#     must define a random_start:int for function to run\n",
    "        Xi = df_X.iloc[random_start-X_length:random_start]\n",
    "        yi = df_y.iloc[random_start:random_start+y_length]\n",
    "\n",
    "        return Xi, yi\n",
    "\n",
    "    \n",
    "#     for loop to select ith values from data\n",
    "    for i in range(number_of_sequences):\n",
    "        Xi, yi = get_Xi_yi(df_X_scaled, df_y, sample_list.pop(), X_length, y_length)\n",
    "        X.append(Xi.values.tolist())\n",
    "        y.append(yi.values.tolist())\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7a51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests on D func\n",
    "X,y= get_X_y(df, \n",
    "            X_length=13, # \n",
    "            y_length=13, \n",
    "            number_of_sequences=51, \n",
    "            number_of_targets=4, \n",
    "            val=True, \n",
    "            val_cutoff=0.8)\n",
    "X.shape, y.shape #((51, 13, 310), (51, 13))\n",
    "\n",
    "y_length = 3\n",
    "X_length = 3\n",
    "val_cutoff = .999\n",
    "sample_list = list(range(int(len(df_y)*val_cutoff), int(len(df_y)-y_length))) #y_length pans the end\n",
    "sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1cf2a19a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Randomstart excercise\n",
    "\n",
    "#random.shuffle(list(range(length-min_size)))\n",
    "#randslicestart = False\n",
    "#start=random.choice(list(range(length-min_size))\n",
    "#if randslicestart:\n",
    "#slic = conc.iloc[randstart:randstart+13]\n",
    "#if not randslicestart:\n",
    "#slic = conc.iloc[0:13]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "356b3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = fulldfslices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cdb8eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 13, 26, 39, 52], 13, 4)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlen= len(data[2]) #len(df)\n",
    "slice_size = 13\n",
    "fullslices = xlen//slice_size\n",
    "slicind = list(e for e in range(xlen) if e%slice_size==0)\n",
    "randslic = random.choice(slicind)\n",
    "slicind, randslic, fullslices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "207bc0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slices = [] \n",
    "for i in randslic:\n",
    "    #slices.append(data[2].iloc[i:i+slice_size])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba5fef",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "3771c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fulldfpad(nsubjects=16, Xymerge=True):\n",
    "    '''Files must be labelled as {subject#}_123.npz' and should be inside a Data folder within the Project.\n",
    "    Returns a list prepared to pad with 720,~18,311 np.arrays\n",
    "    Xymerge=True includes 'y' on the DF\n",
    "    '''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])    \n",
    "    \n",
    "    def gatherdatapad(X, y, Xymerge):\n",
    "        Xyframes=[]\n",
    "        for i in range(45):\n",
    "            if Xymerge:\n",
    "                merge = pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1)\n",
    "                Xyframes.append(np.array(merge))\n",
    "            if not Xymerge:\n",
    "                Xyframes.append(pd.DataFrame(X[i]))\n",
    "        #XyDF = pd.concat(Xyframes) #DFintegrated\n",
    "        return Xyframes #list of pd.DF\n",
    "    \n",
    "    Xy16_list = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        #apply all data to the gather data func to create lists of DFs \n",
    "        Xy = gatherdatapad(data16[i], label16[i],Xymerge=Xymergeg)\n",
    "        Xy16_list += Xy\n",
    "    #XyDF = pd.concat(Xy16_list)\n",
    "    #XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return Xy16_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "32f172e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y np.array (720,), 1 value per array\n",
    "def y_unique(nsubjects=16):\n",
    "    '''y for RNN. after X is padded, this y is used to fit'''\n",
    "    yunique = []\n",
    "    \n",
    "    for i in range(1,nsubjects+1):\n",
    "        y=pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "        for e in range(45):\n",
    "            yunique.append(int(np.unique(y[e])))\n",
    "            \n",
    "    return np.array(yunique).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "60d95534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max1\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#New function to collect all Data across all 16 subjects without split\n",
    "#Full DF no split\n",
    "def fulldfmax(nsubjects=16):\n",
    "    '''returns a np.array shape (720, 74, 310) '''\n",
    "    data16  = [pickle.loads(np.load(f'../data/{i}_123.npz')['data']) for i in range(1,nsubjects+1)]\n",
    "\n",
    "    pad_list=[]\n",
    "    for i in range(nsubjects):\n",
    "        X = list(data16[i].values())\n",
    "        #padding\n",
    "        X_pad = pad_sequences(X, dtype='float32', value=-42069) # int32 by default\n",
    "        pad_list.append(X_pad)\n",
    "\n",
    "    return np.concatenate(pad_list)\n",
    "ddd = fulldfmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8cd690",
   "metadata": {},
   "source": [
    "### Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "02da69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdata(X, y, ntrainbatch=10):\n",
    "    nbatch=ntrainbatch-1\n",
    "    trainframes=[]\n",
    "    testframes =[]\n",
    "    for i in range(45):\n",
    "        if i%15-1<nbatch:\n",
    "            trainframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "        if i%15>nbatch:\n",
    "            testframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "    train = pd.concat(trainframes)\n",
    "    test  = pd.concat(testframes)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def fulldfsplit(nsubjects=16):\n",
    "    '''Train-Test from Full DF function '''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    Xytrain16_list = []\n",
    "    Xytest16_list  = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "            \n",
    "    for i in range(1,nsubjects+1):\n",
    "        #apply all data to the splitdata func to create lists of DFs \n",
    "        train, test = splitdata(data16[i], label16[i], 10)\n",
    "        Xytrain16_list.append(train)\n",
    "        Xytest16_list.append(test)\n",
    "\n",
    "    #create a unified DF from every list with pd.concat(trainframes)\n",
    "    Xytrain16_DF = pd.concat(Xytrain16_list)\n",
    "    Xytest16_DF  = pd.concat(Xytest16_list)\n",
    "    \n",
    "    return Xytrain16_DF, Xytest16_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "47b9216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###slice to even values\n",
    "def allsets(X,y,slice_size=13, trackdict=False,Xymerge=False):\n",
    "    '''adds all possible slices from slice_size  \n",
    "    Xymerge=True returns df with X and y'''\n",
    "    slices = []\n",
    "    dicc={}\n",
    "    for i in range(len(X)):\n",
    "        #conc = pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1)\n",
    "        #length   = len(conc)\n",
    "        if Xymerge:\n",
    "            merge = pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1)\n",
    "            #Xyframes.append(np.array(merge))\n",
    "        if not Xymerge:\n",
    "            merge = pd.DataFrame(X[i])\n",
    "        length   = len(merge)\n",
    "        sobrantes, setscomp = length%slice_size, length//slice_size\n",
    "        for e in range(setscomp):\n",
    "            slic = merge.iloc[slice_size*e:min_size*e+slice_size]\n",
    "            slices.append(np.array(slic))\n",
    "            #dicc[f\"clip {i}\"]=f\"slices:{setscomp}\" #dicc {clip,slice}\n",
    "    #df = pd.concat(slices)\n",
    "    if trackdict:\n",
    "        return slices, dicc\n",
    "    if not trackdict:\n",
    "        return slices #list with 45 sliced arrays\n",
    "#.----------\n",
    "    def gatherdatapad(X, y, Xymerge):\n",
    "        Xyframes=[]\n",
    "        for i in range(45):\n",
    "            if Xymerge:\n",
    "                merge = pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1)\n",
    "                Xyframes.append(np.array(merge))\n",
    "            if not Xymerge:\n",
    "                Xyframes.append(pd.DataFrame(X[i]))\n",
    "        #XyDF = pd.concat(Xyframes) #DFintegrated\n",
    "        return Xyframes #list of pd.DF\n",
    "#.----------\n",
    "    Xy16_list = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        #apply all data to the gather data func to create lists of DFs \n",
    "        Xy = gatherdatapad(data16[i], label16[i],Xymerge=Xymergeg)\n",
    "        Xy16_list += Xy\n",
    "    #XyDF = pd.concat(Xy16_list)\n",
    "    #XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return Xy16_list\n",
    "#.----------\n",
    "#13-slices\n",
    "def fulldfslices(nsubjects=16, slice_size =13, trackdict=False, Xymerge=False):\n",
    "    '''Files must be labelled as {subject#}_123.npz' and should be inside a Data folder within the Project.\n",
    "    slice_size is the desired row length of each slice\n",
    "    '''\n",
    "    Xymerg=Xymerge\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    #trackdic = trackdict\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files into a Dict using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "            \n",
    "    Xy16_list = []\n",
    "    dicc16  = {}\n",
    "    for i in range(1,nsubjects+1): \n",
    "        #apply all data to the gather data func to create lists of DFs \n",
    "        if trackdict:\n",
    "            Xy,dicc = allsets(data16[i], label16[i], slice_size, trackdict=True,Xymerge=Xymerg )\n",
    "            Xy16_list+=Xy\n",
    "            dicc16[f\"subject {i}\"]=dicc  #list with dicc {clip,slice}\n",
    "        if not trackdict:\n",
    "            Xy = allsets(data16[i], label16[i], slice_size, Xymerge=Xymerg)\n",
    "            Xy16_list+=Xy\n",
    "\n",
    "    #XyDF = pd.concat(Xy16_list)\n",
    "    #XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return Xy16_list #list of all slices\n",
    "\n",
    "def fulldfsplit(nsubjects=16):\n",
    "    '''Train-Test from Full DF function '''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    Xytrain16_list = []\n",
    "    Xytest16_list  = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "            \n",
    "    for i in range(1,nsubjects+1):\n",
    "        #apply all data to the splitdata func to create lists of DFs \n",
    "        train, test = splitdata(data16[i], label16[i], 10)\n",
    "        Xytrain16_list.append(train)\n",
    "        Xytest16_list.append(test)\n",
    "\n",
    "    #create a unified DF from every list with pd.concat(trainframes)\n",
    "    Xytrain16_DF = pd.concat(Xytrain16_list)\n",
    "    Xytest16_DF  = pd.concat(Xytest16_list)\n",
    "    \n",
    "    return Xytrain16_DF, Xytest16_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "454199c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xylist = fulldfslices(Xymerge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "738b2c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1904, (13, 311))"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(Xylist), Xylist[0].shape #(1904, (13, 311))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
