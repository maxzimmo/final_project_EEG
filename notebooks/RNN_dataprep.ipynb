{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b558342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9af1bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERNAL FUNCS\n",
    "def splitdata(data, label, ntrainbatch=10):\n",
    "    nbatch=ntrainbatch-1\n",
    "    trainframes=[]\n",
    "    testframes =[]\n",
    "    for i in range(45):\n",
    "        if i%15-1<nbatch:\n",
    "            trainframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "        if i%15>nbatch:\n",
    "            testframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "    train = pd.concat(trainframes)\n",
    "    test  = pd.concat(testframes)\n",
    "    return train, test\n",
    "\n",
    "def gatherdata(X, y):\n",
    "    Xyframes=[]\n",
    "    for i in range(45):\n",
    "        Xyframes.append(pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1))\n",
    "    XyDF = pd.concat(Xyframes)\n",
    "    return XyDF\n",
    "\n",
    "def allsets(X,y,slice_size=13, trackdict=False):\n",
    "    slices = []\n",
    "    dicc={}\n",
    "    for i in range(45):\n",
    "        conc = pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1)\n",
    "        length   = len(conc)\n",
    "        sobrantes, setscomp = length%slice_size, length//slice_size\n",
    "        \n",
    "        for e in range(setscomp):\n",
    "            slic = conc.iloc[slice_size*e:min_size*e+slice_size]\n",
    "            slices.append(slic)\n",
    "            dicc[f\"clip {i}\"]=f\"slices:{setscomp}\" #dicc {clip,slice}\n",
    "    df = pd.concat(slices)\n",
    "    if trackdict:\n",
    "        return df, dicc\n",
    "    if not trackdict:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c50e602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL FUNCS\n",
    "\n",
    "#Train-Test from Full DF function \n",
    "def fulldfsplit(nsubjects=16):\n",
    "    '''Files must be labelled as {subject#}_123.npz' and should be inside a Data folder within the Project'''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    Xytrain16_list = []\n",
    "    Xytest16_list  = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "            \n",
    "    for i in range(1,nsubjects+1):\n",
    "        #apply all data to the splitdata func to create lists of DFs \n",
    "        train, test = splitdata(data16[i], label16[i], 10)\n",
    "        Xytrain16_list.append(train)\n",
    "        Xytest16_list.append(test)\n",
    "\n",
    "    #create a unified DF from every list with pd.concat(trainframes)\n",
    "    Xytrain16_DF = pd.concat(Xytrain16_list)\n",
    "    Xytest16_DF  = pd.concat(Xytest16_list)\n",
    "    \n",
    "    return Xytrain16_DF, Xytest16_DF\n",
    "\n",
    "#Full DF no split\n",
    "def fulldf(nsubjects=16):\n",
    "    '''Files must be labelled as {subject#}_123.npz' and should be inside a Data folder within the Project.'''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "    Xy16_list = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        #apply all data to the gather data func to create lists of DFs \n",
    "        Xy = gatherdata(data16[i], label16[i])\n",
    "        Xy16_list.append(Xy)\n",
    "    XyDF = pd.concat(Xy16_list)\n",
    "    XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return XyDF\n",
    "\n",
    "#Full DF, no split, slicing each clip to multiples of 13\n",
    "def fulldfslices(nsubjects=16, slice_size =13, trackdict=False):\n",
    "    '''Files must be labelled as {subject#}_123.npz' and should be inside a Data folder within the Project.\n",
    "    slice_size is the desired row length of each slice\n",
    "    '''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    #trackdic = trackdict\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "    Xy16_list = []\n",
    "    dicc16  = {}\n",
    "    for i in range(1,nsubjects+1): \n",
    "        #apply all data to the gather data func to create lists of DFs \n",
    "        if trackdict:\n",
    "            Xy,dicc = allsets(data16[i], label16[i], slice_size, trackdict=True)\n",
    "            Xy16_list.append(Xy)\n",
    "            dicc16[f\"subject {i}\"]=dicc  #list with dicc {clip,slice}\n",
    "        if not trackdict:\n",
    "            Xy = allsets(data16[i], label16[i], slice_size)\n",
    "            Xy16_list.append(Xy)\n",
    "\n",
    "    XyDF = pd.concat(Xy16_list)\n",
    "    XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return XyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef2e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dayus formula\n",
    "def get_X_y(df, \n",
    "            X_length=13, # \n",
    "            y_length=13, \n",
    "            number_of_sequences=51, \n",
    "            number_of_targets=1, \n",
    "            val=False, \n",
    "            val_cutoff=0.8):\n",
    "\n",
    "\n",
    "#     limit dataframes to length for train/test splits\n",
    "    df_X = df.copy().drop(columns=df.columns[-1], axis=1)\n",
    "    df_y = df.iloc[:, -1].copy()\n",
    "    \n",
    "#     convert and scale X dataframe to PCA to solve dimensionality problem\n",
    "    scaler = MinMaxScaler()\n",
    "    df_X_scaled = pd.DataFrame(scaler.fit_transform(df_X), columns=df_X.columns, index=df_X.index)\n",
    "    \n",
    "\n",
    "#     create unique list to sample random datapoints from\n",
    "    if val:\n",
    "        sample_list = list(range(int(len(df_y)*val_cutoff), int(len(df_y)-y_length))) #y_length pans the end\n",
    "    if not val:\n",
    "        sample_list = list(range(int(X_length), int(len(df_y)-y_length))) #X_length pans start\n",
    "    random.shuffle(sample_list)\n",
    "    \n",
    "#     empty lists to append data to, will create 3D dataframe here\n",
    "    X, y = [], []\n",
    "    \n",
    "    \n",
    "#     define a simple data slicing and selection function. This function will create a slice of data from a specified random starting position. The random position must be generated externally.\n",
    "    \n",
    "    def get_Xi_yi(df_X, \n",
    "              df_y,\n",
    "              random_start, #list of random values\n",
    "              X_length, #X_length pans start\n",
    "              y_length #y_length pans the end\n",
    "                 ): \n",
    "        '''Define a simple data slicing and selection function. \n",
    "        This function will create a slice of data from a specified random starting position. \n",
    "        The random position must be generated externally.'''\n",
    "    \n",
    "#     must define a random_start:int for function to run\n",
    "        Xi = df_X.iloc[random_start-X_length:random_start]\n",
    "        yi = df_y.iloc[random_start:random_start+y_length]\n",
    "\n",
    "        return Xi, yi\n",
    "\n",
    "    \n",
    "#     for loop to select ith values from data\n",
    "    for i in range(number_of_sequences):\n",
    "        Xi, yi = get_Xi_yi(df_X_scaled, df_y, sample_list.pop(), X_length, y_length)\n",
    "        X.append(Xi.values.tolist())\n",
    "        y.append(yi.values.tolist())\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7a51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests on D func\n",
    "X,y= get_X_y(df, \n",
    "            X_length=13, # \n",
    "            y_length=13, \n",
    "            number_of_sequences=51, \n",
    "            number_of_targets=4, \n",
    "            val=True, \n",
    "            val_cutoff=0.8)\n",
    "X.shape, y.shape #((51, 13, 310), (51, 13))\n",
    "\n",
    "y_length = 3\n",
    "X_length = 3\n",
    "val_cutoff = .999\n",
    "sample_list = list(range(int(len(df_y)*val_cutoff), int(len(df_y)-y_length))) #y_length pans the end\n",
    "sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1cf2a19a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Randomstart excercise\n",
    "\n",
    "#random.shuffle(list(range(length-min_size)))\n",
    "#randslicestart = False\n",
    "#start=random.choice(list(range(length-min_size))\n",
    "#if randslicestart:\n",
    "#slic = conc.iloc[randstart:randstart+13]\n",
    "#if not randslicestart:\n",
    "#slic = conc.iloc[0:13]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "356b3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = fulldfslices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cdb8eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 13, 26, 39, 52], 13, 4)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "207bc0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slices = [] \n",
    "for i in randslic:\n",
    "    #slices.append(data[2].iloc[i:i+slice_size])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba5fef",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "3771c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fulldfpad(nsubjects=16, Xymerge=True):\n",
    "    '''Files must be labelled as {subject#}_123.npz' and should be inside a Data folder within the Project.\n",
    "    Returns a list prepared to pad with 720,~18,311 np.arrays\n",
    "    Xymerge=True includes 'y' on the DF\n",
    "    '''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])    \n",
    "    \n",
    "    def gatherdatapad(X, y, Xymerge):\n",
    "        Xyframes=[]\n",
    "        for i in range(45):\n",
    "            if Xymerge:\n",
    "                merge = pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1)\n",
    "                Xyframes.append(np.array(merge))\n",
    "            if not Xymerge:\n",
    "                Xyframes.append(pd.DataFrame(X[i]))\n",
    "        #XyDF = pd.concat(Xyframes) #DFintegrated\n",
    "        return Xyframes #list of np.arrays\n",
    "    \n",
    "    Xy16_list = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        #apply all data to the gather data func to create lists of DFs \n",
    "        Xy = gatherdatapad(data16[i], label16[i],Xymerge=Xymergeg)\n",
    "        Xy16_list += Xy\n",
    "    #XyDF = pd.concat(Xy16_list)\n",
    "    #XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return Xy16_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32f172e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y np.array (720,), 1 value per array\n",
    "def y_unique(nsubjects=16):\n",
    "    '''y for RNN. after X is padded, this y is used to fit'''\n",
    "    yunique = []\n",
    "    \n",
    "    for i in range(1,nsubjects+1):\n",
    "        y=pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "        for e in range(45):\n",
    "            yunique.append(int(np.unique(y[e])))\n",
    "            \n",
    "    return np.array(yunique).astype(np.float32)\n",
    "y = y_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60d95534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max1\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#New function to collect all Data across all 16 subjects without split\n",
    "#Full DF no split\n",
    "def fulldfmax(nsubjects=16):\n",
    "    '''returns a np.array shape (720, 74, 310) '''\n",
    "    data16  = [pickle.loads(np.load(f'../data/{i}_123.npz')['data']) for i in range(1,nsubjects+1)]\n",
    "\n",
    "    pad_list=[]\n",
    "    for i in range(nsubjects):\n",
    "        X = list(data16[i].values())\n",
    "        #padding\n",
    "        X_pad = pad_sequences(X, dtype='float32', value=-42069) # int32 by default\n",
    "        pad_list.append(X_pad)\n",
    "\n",
    "    return np.concatenate(pad_list)\n",
    "X = fulldfmax()\n",
    "#    return pad_list\n",
    "#Xlist=fulldfmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb1012fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((720, 74, 310), (720,), 720)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd.shape, yyy.shape, ddd.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "828bb8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ddd), type(yyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvtsplit(X,y,train_size=.7, val_size=.2):\n",
    "    '''X.shape, y.shape = ((720, 74, 310), (720,)\n",
    "    train & val should be percentage values between 0-1'''\n",
    "    #if X.shape[0]== y.shape[0]:\n",
    "    n = X.shape[0]\n",
    "    ntrain, nval, ntest = int(train_size*n), int(val_size*n), int((1-train_size-val_size)*n)\n",
    "    sample_list=[e for e in range(n)] #n-arrays size list\n",
    "    random.shuffle(sample_list)\n",
    "    \n",
    "    random_train = sample_list[:ntrain]\n",
    "    random_val   = sample_list[ntrain:ntrain+nval]\n",
    "    random_test  = sample_list[ntrain+nval:]\n",
    "    \n",
    "    for e in range(total_frames):\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5eed8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpt func\n",
    "def RNN_split_data(X, y, train_size, val_size, random_state=42):\n",
    "    '''Takes fulldfmax() as X, y_unique() as y. '''\n",
    "    test_size = 1-train_size-val_size\n",
    "    assert train_size + val_size + test_size == 1.0, \"Sizes must add up to 1.0\"\n",
    "    #assert abs(train_size + val_size + test_size - 1.0) < 1e-9, \"Sizes must add up to 1.0\"\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Calculate total size and generate a permutation\n",
    "    total_size = X.shape[0] #720\n",
    "    permutation = np.random.permutation(total_size) #random sequence of 720 values as array (720,)\n",
    "\n",
    "    # Shuffle X and y\n",
    "    X = X[permutation]\n",
    "    y = y[permutation]\n",
    "\n",
    "    # Calculate the indices for the splits\n",
    "    train_end = int(total_size * train_size)\n",
    "    val_end = train_end + int(total_size * val_size)\n",
    "\n",
    "    # Split the X array\n",
    "    X_train = X[:train_end]\n",
    "    X_val = X[train_end:val_end]\n",
    "    X_test = X[val_end:]\n",
    "\n",
    "    # Split the y array\n",
    "    y_train = y[:train_end]\n",
    "    y_val = y[train_end:val_end]\n",
    "    y_test = y[val_end:]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b9fbaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = RNN_split_data(X, y, 0.7, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0b84fed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((503, 74, 310), (144, 74, 310), (73, 74, 310), (503,), (144,), (73,))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "24d47952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "503+144+73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bef96312",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "np.random.seed(random_state)\n",
    "total_size = X.shape[0]\n",
    "permutation = np.random.permutation(total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "225fe7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#permutation.shape\n",
    "train_size, val_size, test_size = 0.7, 0.2, 0.1\n",
    "print(train_size+ val_size+ test_size)\n",
    "train_size, val_size = 0.7, 0.2\n",
    "test_size = 1-train_size- val_size\n",
    "print(train_size+ val_size+ test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf57ad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 12, 0, 2, 18, 10, 7, 11, 16, 8, 1, 19, 15, 13, 9, 3, 14, 17, 4, 6]\n",
      "14 4 2\n",
      "[5, 12, 0, 2, 18, 10, 7, 11, 16, 8, 1, 19, 15, 13] [9, 3, 14, 17] [4, 6]\n",
      "14 4 2\n"
     ]
    }
   ],
   "source": [
    "n=20\n",
    "ll=[e for e in range(n)]\n",
    "random.shuffle(ll)\n",
    "ptrain, pval = .7,.2\n",
    "ptest = 1-ptrain-pval\n",
    "train, val, test = int(ptrain*n), int(pval*n), int(ptest*n)\n",
    "\n",
    "ttrain=ll[0:train]\n",
    "tval=ll[train:train+val]\n",
    "ttest=ll[train+val:]\n",
    "print(ll)\n",
    "print(train, val, test)\n",
    "print(ttrain, tval, ttest)\n",
    "print(len(ttrain), len(tval), len(ttest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6b8c471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlen= len(data[2]) #len(df)\n",
    "slice_size = 13\n",
    "fullslices = xlen//slice_size\n",
    "slicind = list(e for e in range(xlen) if e%slice_size==0)\n",
    "randslic = random.choice(slicind)\n",
    "slicind, randslic, fullslices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8cd690",
   "metadata": {},
   "source": [
    "### Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "02da69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdata(X, y, ntrainbatch=10):\n",
    "    nbatch=ntrainbatch-1\n",
    "    trainframes=[]\n",
    "    testframes =[]\n",
    "    for i in range(45):\n",
    "        if i%15-1<nbatch:\n",
    "            trainframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "        if i%15>nbatch:\n",
    "            testframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "    train = pd.concat(trainframes)\n",
    "    test  = pd.concat(testframes)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def fulldfsplit(nsubjects=16):\n",
    "    '''Train-Test from Full DF function '''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    Xytrain16_list = []\n",
    "    Xytest16_list  = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files data into a Dict named 'i_123.npz' using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "            \n",
    "    for i in range(1,nsubjects+1):\n",
    "        #apply all data to the splitdata func to create lists of DFs \n",
    "        train, test = splitdata(data16[i], label16[i], 10)\n",
    "        Xytrain16_list.append(train)\n",
    "        Xytest16_list.append(test)\n",
    "\n",
    "    #create a unified DF from every list with pd.concat(trainframes)\n",
    "    Xytrain16_DF = pd.concat(Xytrain16_list)\n",
    "    Xytest16_DF  = pd.concat(Xytest16_list)\n",
    "    \n",
    "    return Xytrain16_DF, Xytest16_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b9216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###slice to even values\n",
    "def allsets(X,y,slice_size=13, trackdict=False,Xymerge=False):\n",
    "    '''adds all possible slices from slice_size  \n",
    "    Xymerge=True returns df with X and y'''\n",
    "    slices = []\n",
    "    dicc={}\n",
    "    for i in range(len(X)):\n",
    "        #conc = pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1)\n",
    "        #length   = len(conc)\n",
    "        if Xymerge:\n",
    "            merge = pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1)\n",
    "            #Xyframes.append(np.array(merge))\n",
    "        if not Xymerge:\n",
    "            merge = pd.DataFrame(X[i])\n",
    "        length   = len(merge)\n",
    "        sobrantes, setscomp = length%slice_size, length//slice_size\n",
    "        for e in range(setscomp):\n",
    "            slic = merge.iloc[slice_size*e:slice_size*e+slice_size]\n",
    "            slices.append(np.array(slic))\n",
    "            #dicc[f\"clip {i}\"]=f\"slices:{setscomp}\" #dicc {clip,slice}\n",
    "    #df = pd.concat(slices)\n",
    "    if trackdict:\n",
    "        return slices, dicc\n",
    "    if not trackdict:\n",
    "        return slices #list with 45 sliced arrays\n",
    "#.----------\n",
    "    def gatherdatapad(X, y, Xymerge):\n",
    "        Xyframes=[]\n",
    "        for i in range(45):\n",
    "            if Xymerge:\n",
    "                merge = pd.concat([pd.DataFrame(X[i]), pd.DataFrame(y[i])], axis=1)\n",
    "                Xyframes.append(np.array(merge))\n",
    "            if not Xymerge:\n",
    "                Xyframes.append(pd.DataFrame(X[i]))\n",
    "        #XyDF = pd.concat(Xyframes) #DFintegrated\n",
    "        return Xyframes #list of np.arrays\n",
    "#.----------\n",
    "    Xy16_list = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        #apply all data to the gather data func to create lists of DFs \n",
    "        Xy = gatherdatapad(data16[i], label16[i],Xymerge=Xymergeg)\n",
    "        Xy16_list += Xy\n",
    "    #XyDF = pd.concat(Xy16_list)\n",
    "    #XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return Xy16_list\n",
    "#.----------\n",
    "#13-slices\n",
    "def fulldfslices(nsubjects=16, slice_size =13, trackdict=False, Xymerge=False):\n",
    "    '''Files must be labelled as {subject#}_123.npz' and should be inside a Data folder within the Project.\n",
    "    slice_size is the desired row length of each slice\n",
    "    trackdict returns a dict with the slices per clip. To retrieve: df, dicc = fulldfslices(trackdict=True)  \n",
    "    Xymerge=True returns the df with both X+y'''\n",
    "    Xymerg=Xymerge\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    #trackdic = trackdict\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Load all 16 files into a Dict using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "            \n",
    "    Xy16_list = []\n",
    "    dicc16  = {}\n",
    "    for i in range(1,nsubjects+1): \n",
    "        #apply all data to the gather data func to create lists of DFs \n",
    "        if trackdict:\n",
    "            Xy,dicc = allsets(data16[i], label16[i], slice_size, trackdict=True,Xymerge=Xymerg )\n",
    "            Xy16_list+=Xy\n",
    "            dicc16[f\"subject {i}\"]=dicc  #list with dicc {clip,slice}\n",
    "        if not trackdict:\n",
    "            Xy = allsets(data16[i], label16[i], slice_size, Xymerge=Xymerg)\n",
    "            Xy16_list+=Xy\n",
    "\n",
    "    #XyDF = pd.concat(Xy16_list)\n",
    "    #XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return Xy16_list #list of all slices\n",
    "\n",
    "def fulldfsplit(nsubjects=16):\n",
    "    '''Train-Test from Full DF function '''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    Xytrain16_list = []\n",
    "    Xytest16_list  = []\n",
    "    for i in range(1,nsubjects+1): \n",
    "        # Consolidate all 16 files into a list using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "            \n",
    "    for i in range(1,nsubjects+1):\n",
    "        #apply all data to the splitdata func to create lists of DFs \n",
    "        train, test = splitdata(data16[i], label16[i], 10)\n",
    "        Xytrain16_list.append(train)\n",
    "        Xytest16_list.append(test)\n",
    "\n",
    "    #create a unified DF from every list with pd.concat(trainframes)\n",
    "    Xytrain16_DF = pd.concat(Xytrain16_list)\n",
    "    Xytest16_DF  = pd.concat(Xytest16_list)\n",
    "    \n",
    "    return Xytrain16_DF, Xytest16_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454199c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xylist = fulldfslices(Xymerge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "738b2c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1904, (13, 310))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xylist), Xylist[0].shape #(1904, (13, 311))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "492a12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_unique(nsubjects=16):\n",
    "    '''y for RNN. after X is padded, this y is used to fit'''\n",
    "    yunique = []\n",
    "    for i in range(1,nsubjects+1):\n",
    "        y=pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "        for e in range(45):\n",
    "            yunique.append(int(np.unique(y[e])))\n",
    "    return np.array(yunique).astype(np.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b26f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def access16(y=True):\n",
    "    '''converts Person file into a dict with 16 keys.\n",
    "    if y: returns data16 and label16.\n",
    "    if not y: returns only data16'''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    for i in range(1,nsubjects+1):\n",
    "        # Load all 16 files data into a Dict named ‘i_123.npz’ using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        if y:\n",
    "            label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])  \n",
    "    if y:\n",
    "        return data16, label16\n",
    "    if not y:\n",
    "        return data16\n",
    "    \n",
    "def rnn_df(nsubjects=16):\n",
    "    '''returns a list with 16 arrays'''\n",
    "    data16  = [pickle.loads(np.load(f'../data/{i}_123.npz')['data']) for i in range(1,nsubjects+1)]\n",
    "    return data16\n",
    "\n",
    "def concate(X,y):\n",
    "    ''' joins X-frame and y-frame. returns DF '''\n",
    "    Xyframe = pd.concat([pd.DataFrame(X), pd.DataFrame(y)], axis=1)\n",
    "    return Xyframe\n",
    "    \n",
    "def gatherdata(X, y):\n",
    "    ''' X,y = inputs Dicts with 45 keys e.g. data[1], label[1]\n",
    "    returns a pd.DF with all 45 clips concatenated (1823, 311))'''\n",
    "    Xyframes=[]\n",
    "    for i in range(45):\n",
    "        Xyframes.append(concate(X[i],y[i]))\n",
    "    XyDF = pd.concat(Xyframes)\n",
    "    return XyDF\n",
    "def fulldf(nsubjects=16):\n",
    "    '''returns a pd.DF with X and y. y labelled as 'target' '''\n",
    "    #get files into dicts\n",
    "    data16, label16 = access16()\n",
    "    #apply all data to the gather data func to create lists of DFs\n",
    "    Xy16_list = [gatherdata(data16[e], label16[e]) for e in range(1,nsubjects+1)]\n",
    "    #concat clips one on top of the other one\n",
    "    XyDF = pd.concat(Xy16_list)\n",
    "    XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return XyDF   #pd.DF with X and y. y labelled as 'target'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
