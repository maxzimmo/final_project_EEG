{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04526ccd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b6bb39",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.4\n",
      "4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 15:31:41.990591: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns \n",
    "import random\n",
    "\n",
    "# check the version of these modules\n",
    "print(np.__version__)\n",
    "print(pickle.format_version)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfa934",
   "metadata": {},
   "source": [
    "# Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d89c61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'label']\n"
     ]
    }
   ],
   "source": [
    "# load DE features named '1_123.npz'\n",
    "data_npz = np.load('../data/1_123.npz')\n",
    "print(data_npz.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f3df53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44])\n",
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'loads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(label\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 13\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/numpy/__init__.py:311\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tester\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'loads'"
     ]
    }
   ],
   "source": [
    "# get data and label\n",
    "# ** both 'data' and 'label' are pickled dict **\n",
    "\n",
    "data = pickle.loads(data_npz['data'])\n",
    "label = pickle.loads(data_npz['label'])\n",
    "\n",
    "label_dict = {0:'Disgust', 1:'Fear', 2:'Sad', 3:'Neutral', 4:'Happy'}\n",
    "\n",
    "print(data.keys())\n",
    "print(label.keys())\n",
    "\n",
    "\n",
    "np.loads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d01b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ea8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, there are 45 keys in both 'data' and 'label'.\n",
    "# Each participant took part in our experiments for 3 sessions, and he/she watched 15 movie clips (i.e. 15 trials) during each session.\n",
    "# Therefore, we could extract 3 * 15 = 45 DE feature matrices.\n",
    "\n",
    "# The key indexes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] belong to Session 1.\n",
    "# The key indexes [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] belong to Session 2.\n",
    "# The key indexes [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44] belong to Session 3.\n",
    "\n",
    "# We will print the emotion labels for each trial.\n",
    "label_dict = {0:'Disgust', 1:'Fear', 2:'Sad', 3:'Neutral', 4:'Happy'}\n",
    "for i in range(45):\n",
    "    print('Session {} -- Trial {} -- EmotionLabel : {}'.format(i//15+1, i%15+1, label_dict[label[i][0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5d09b",
   "metadata": {},
   "source": [
    "# Transforming dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423919e",
   "metadata": {},
   "source": [
    "## Transform dataset to 2D Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d200b",
   "metadata": {},
   "source": [
    "<u>Here we transform the data in a train and test split, mainly for our baseline model SVM:</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data Function\n",
    "def splitdata(data, label, ntrainbatch):\n",
    "    nbatch=ntrainbatch-1\n",
    "    trainframes=[]\n",
    "    testframes =[]\n",
    "    for i in range(45):\n",
    "        if i%15-1<nbatch:\n",
    "            trainframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "        if i%15>nbatch:\n",
    "            testframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "    train = pd.concat(trainframes)\n",
    "    test  = pd.concat(testframes)\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce424ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fulldfsplit() function for Train-Test Split with Full DF\n",
    "def fulldfsplit(nsubjects=16):\n",
    "    '''Files must be labelled as {subject#}_123.npz’ and should be inside a Data folder within the Project'''\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    Xytrain16_list = []\n",
    "    Xytest16_list  = []\n",
    "    for i in range(1,nsubjects+1):\n",
    "        # Load all 16 files data into a Dict named ‘i_123.npz’ using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "    for i in range(1,nsubjects+1):\n",
    "        #apply all data to the splitdata func to create lists of DFs\n",
    "        train, test = splitdata(data16[i], label16[i], 10)\n",
    "        Xytrain16_list.append(train)\n",
    "        Xytest16_list.append(test)\n",
    "    #create a unified DF from every list with pd.concat(trainframes)\n",
    "    Xytrain16_DF = pd.concat(Xytrain16_list)\n",
    "    Xytest16_DF  = pd.concat(Xytest16_list)\n",
    "    return Xytrain16_DF, Xytest16_DF\n",
    "Xtrain,Xtest = fulldfsplit()\n",
    "Xtrain.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a7850",
   "metadata": {},
   "source": [
    "<u> Here we concatenate all the trials together as well as add the target into one whole dataframe! </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f81de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherdata(X, y):\n",
    "    Xyframes=[]\n",
    "    for i in range(45):\n",
    "        Xyframes.append(pd.concat([pd.DataFrame(data[i]), pd.DataFrame(label[i])], axis=1))\n",
    "    XyDF = pd.concat(Xyframes)\n",
    "    return XyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261caf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New function to collect all Data across all 16 subjects without split\n",
    "#Full DF no split\n",
    "def fulldf(nsubjects=16):\n",
    "    data16  = {}\n",
    "    label16 = {}\n",
    "    for i in range(1,nsubjects+1):\n",
    "        # Load all 16 files data into a Dict named ‘i_123.npz’ using a for loop\n",
    "        data16[i]  = pickle.loads(np.load(f'../data/{i}_123.npz')['data'])\n",
    "        label16[i] = pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "    Xy16_list = []\n",
    "    for i in range(1,nsubjects+1):\n",
    "        #apply all data to the gather data func to create lists of DFs\n",
    "        Xy = gatherdata(data16[i], label16[i])\n",
    "        Xy16_list.append(Xy)\n",
    "    XyDF = pd.concat(Xy16_list)\n",
    "    XyDF.columns = [*XyDF.columns[:-1], 'target']\n",
    "    return XyDF\n",
    "Xytotal = fulldf()\n",
    "Xytotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1d438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xytotal = Xytotal.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55308337",
   "metadata": {},
   "source": [
    "<u> Here we create an an numpy array, containing the input data to the RNN </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93269b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_df(nsubjects=16):\n",
    "    data16  = [pickle.loads(np.load(f'../data/{i}_123.npz')['data']) for i in range(1,nsubjects+1)]\n",
    "    return data16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c6862",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd6f6a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Rachels notebook EDA 06 June for heatmap "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5162292",
   "metadata": {},
   "source": [
    "# Preprocessing of data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd30f70",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Basemodel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c64a89",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Xtrain.columns = [*Xtrain.columns[:-1], 'target']\n",
    "Xtest.columns = [*Xtest.columns[:-1], 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d276e75",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# splitting test and train\n",
    "X_test = Xtest.iloc[:, :-1].values\n",
    "y_test = Xtest.iloc[:, -1].values\n",
    "X_train = Xtrain.iloc[:, :-1].values\n",
    "y_train = Xtrain.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d53978",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# scaling of the dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_M = MinMaxScaler()\n",
    "\n",
    "scaler_M.fit(X_train)\n",
    "X_train_minmaxed = scaler_M.transform(X_train)\n",
    "\n",
    "scaler_M.fit(X_test)\n",
    "X_test_minmaxed = scaler_M.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd01fd6",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86258a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scaling of the dataset for full: here we take the full take the full data set and fit it on a MinMax scaler \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.9)\n",
    "scaler_com = MinMaxScaler()\n",
    "scaler_com.fit(Xytotal)\n",
    "pca.fit(Xytotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b313daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, we load only the ndarray of the experiments values, and transform each trial with the previously fitted scaler of the whole dataset \n",
    "ddd = rnn_df()\n",
    "lst = []\n",
    "for i in range(16): \n",
    "    each_participant = list(ddd[i].values())\n",
    "    for j in each_participant: \n",
    "        sequence_scaled = scaler_com.transform(j)\n",
    "        pca_sequence = pca.transform(sequence_scaled)\n",
    "        lst.append(pca_sequence)\n",
    "# afterwards, we pad the whole dataset, so we have a uniformous input to our dataset - therefore, we can use the RNN \n",
    "X_pad = pad_sequences(lst, value=-42069, padding=\"post\", dtype='float32') # int32 by default\n",
    "X_pad.shape\n",
    "\n",
    "X_pad[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y value needs to be one hot encoded for the RNN: therefore, we first take the single value of each trial to a an ndarray and reshape it \n",
    "yunique = []\n",
    "nsubjects=16\n",
    "for i in range(1,nsubjects+1):\n",
    "    y=pickle.loads(np.load(f'../data/{i}_123.npz')['label'])\n",
    "    for e in range(45):\n",
    "        yunique.append(int(np.unique(y[e])))\n",
    "y = np.array(yunique)\n",
    "y\n",
    "\n",
    "y_re = y.reshape(-1, 1)\n",
    "y_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101ce5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# afterwards we OHE the target \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Instantiate the OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False) \n",
    "# Fit encoder\n",
    "ohe.fit(y_re) \n",
    "y_OHE = ohe.transform(y_re)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41892712",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## ARCHIVE: PCA - we do not use PCA, but maybe for future reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b14c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#pca = PCA(n_components=0.9)\n",
    "#pca.fit(X_full_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac969f",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = pca.components_\n",
    "\n",
    "# Print PCs as COLUMNS\n",
    "W = pd.DataFrame(W.T,\n",
    "                 columns=[f'PC{i}' for i in range(1, 12 )])\n",
    "\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44369d99",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47a478",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component'); plt.ylabel('% explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b0af8",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f8440",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check info about dataset \n",
    "\n",
    "# check class distribution of emotion in large data set \n",
    "\n",
    "# create feature analysis: checking for outliers, distribution of numerical features using histograms or boxplots, \n",
    "\n",
    "# correlation analysis between the independent variables \n",
    "\n",
    "# create visualisations to gain further insight "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73204b05",
   "metadata": {},
   "source": [
    "# Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a052376",
   "metadata": {},
   "source": [
    "## Baseline Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b79eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the dataset is balanced and each emotion category is equally shown \n",
    "unique_y = len(np.unique(y_train))\n",
    "baseline_score = 1 / unique_y \n",
    "baseline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ee1b2",
   "metadata": {},
   "source": [
    "## Baseline Model: SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa1e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'kernel': ['rbf'],\n",
    "    'C': [10]\n",
    "}\n",
    "\n",
    "# Create the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "grid_search.fit(X_train_minmaxed, y_train)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred = best_model.predict(X_test_minmaxed)\n",
    "\n",
    "# Evaluate the accuracy of the best model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Accuracy:\", accuracy)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Best Accuracy: 0.6376201923076923\n",
    "# Best Hyperparameters: {'C': 10, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb4c18",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Random Forest Model (if we still have time) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e02084",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input X and y for the RNN, plus input shape \n",
    "input_shape = X_pad.shape[1:]\n",
    "X_rnn = X_pad\n",
    "y_rnn = y_OHE \n",
    "\n",
    "\n",
    "random_list = list(range(720))\n",
    "train = []\n",
    "validation = []\n",
    "test = []\n",
    "for i in range(720): \n",
    "    val = random.choice(random_list) #520 \n",
    "    random_list = [x for x in random_list if x != val] # 719 \n",
    "    if len(random_list) > 216: \n",
    "        train.append(X_rnn[val])\n",
    "    if len (random_list) < 216 and len (random_list) > 36: \n",
    "        validation.append(X_rnn[val]) \n",
    "    else: \n",
    "        test.append(X_rnn[val])\n",
    "\n",
    "np.array(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a98e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_list)\n",
    "random_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106340d1",
   "metadata": {},
   "source": [
    "## RNN Val_accuracy of up to 81% after applying PCA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Model Architecture\n",
    "model = Sequential()\n",
    "model.add(layers.Masking(mask_value=-42069.,input_shape=input_shape))\n",
    "\n",
    "model.add(layers.LSTM(units=20, activation='tanh', return_sequences=True))\n",
    "model.add(layers.LSTM(units=20, activation='tanh', return_sequences=True))\n",
    "#model.add(layers.LSTM(units=10, activation='tanh', return_sequences=True))\n",
    "\n",
    "model.add(layers.LSTM(units=20, activation='tanh', return_sequences=False))\n",
    "\n",
    "#model.add(layers.Dense(20, activation='relu'))\n",
    "#model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "optimizer = AdamW(learning_rate=0.005, \n",
    "                  #weight_decay=0.1, \n",
    "                  beta_1=0.9)\n",
    "\n",
    "# 2. Model Compilation\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "    \n",
    "# –– Fit\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs = 200,         # Notice that we are not using any Early Stopping Criterion\n",
    "    batch_size = 32, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af6257",
   "metadata": {},
   "source": [
    "# Data Storytelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ab900",
   "metadata": {},
   "source": [
    "## How to make a nice presentation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733c816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
